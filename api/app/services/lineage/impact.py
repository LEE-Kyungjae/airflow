"""
Impact Analyzer - ì˜í–¥ ë¶„ì„

ê¸°ëŠ¥:
- ë³€ê²½ ì˜í–¥ ë¶„ì„ (ì†ŒìŠ¤/í•„ë“œ ë³€ê²½ ì‹œ ì˜í–¥ ë²”ìœ„)
- ì˜ì¡´ì„± ë¶„ì„
- ë°ì´í„° ê³„ë³´ ë³´ê³ ì„œ
"""

from datetime import datetime
from typing import Any, Dict, List, Optional, Set
from dataclasses import dataclass, field
from enum import Enum
import logging

from .tracker import LineageTracker, LineageNode, LineageEdge, NodeType, EdgeType
from .graph import LineageGraph

logger = logging.getLogger(__name__)


class ImpactLevel(str, Enum):
    """ì˜í–¥ ìˆ˜ì¤€"""
    NONE = "none"
    LOW = "low"
    MEDIUM = "medium"
    HIGH = "high"
    CRITICAL = "critical"


@dataclass
class ImpactedNode:
    """ì˜í–¥ë°›ëŠ” ë…¸ë“œ"""
    node: LineageNode
    impact_level: ImpactLevel
    impact_reason: str
    distance_from_source: int
    path: List[str] = field(default_factory=list)  # ì˜í–¥ ì „íŒŒ ê²½ë¡œ


@dataclass
class ImpactResult:
    """ì˜í–¥ ë¶„ì„ ê²°ê³¼"""
    source_node_id: str
    change_type: str  # delete, modify, schema_change
    analysis_time: datetime = field(default_factory=datetime.utcnow)

    # ì˜í–¥ë°›ëŠ” ë…¸ë“œ
    impacted_nodes: List[ImpactedNode] = field(default_factory=list)

    # ìš”ì•½
    total_impacted: int = 0
    by_level: Dict[str, int] = field(default_factory=dict)
    by_type: Dict[str, int] = field(default_factory=dict)

    # ê¶Œì¥ ì¡°ì¹˜
    recommendations: List[str] = field(default_factory=list)

    def to_dict(self) -> Dict[str, Any]:
        return {
            "source_node_id": self.source_node_id,
            "change_type": self.change_type,
            "analysis_time": self.analysis_time.isoformat(),
            "total_impacted": self.total_impacted,
            "by_level": self.by_level,
            "by_type": self.by_type,
            "impacted_nodes": [
                {
                    "node_id": n.node.node_id,
                    "name": n.node.name,
                    "type": n.node.node_type.value,
                    "impact_level": n.impact_level.value,
                    "impact_reason": n.impact_reason,
                    "distance": n.distance_from_source,
                    "path": n.path
                }
                for n in self.impacted_nodes
            ],
            "recommendations": self.recommendations
        }


class ImpactAnalyzer:
    """ì˜í–¥ ë¶„ì„ê¸°"""

    def __init__(self, lineage_tracker: LineageTracker):
        """
        Args:
            lineage_tracker: ë¦¬ë‹ˆì§€ ì¶”ì ê¸°
        """
        self.tracker = lineage_tracker

    def analyze_deletion_impact(
        self,
        node_id: str,
        include_recommendations: bool = True
    ) -> ImpactResult:
        """
        ë…¸ë“œ ì‚­ì œ ì˜í–¥ ë¶„ì„

        Args:
            node_id: ì‚­ì œí•  ë…¸ë“œ ID
            include_recommendations: ê¶Œì¥ ì¡°ì¹˜ í¬í•¨ ì—¬ë¶€

        Returns:
            ì˜í–¥ ë¶„ì„ ê²°ê³¼
        """
        result = ImpactResult(
            source_node_id=node_id,
            change_type="delete"
        )

        # ë‹¤ìš´ìŠ¤íŠ¸ë¦¼ ë…¸ë“œ ì¡°íšŒ
        downstream_nodes, downstream_edges = self.tracker.get_downstream(node_id)

        # ì˜í–¥ ë¶„ì„
        impacted = self._analyze_downstream_impact(
            node_id,
            downstream_nodes,
            downstream_edges,
            change_type="delete"
        )

        result.impacted_nodes = impacted
        result.total_impacted = len(impacted)

        # ë ˆë²¨ë³„ ì§‘ê³„
        for node in impacted:
            level = node.impact_level.value
            result.by_level[level] = result.by_level.get(level, 0) + 1
            node_type = node.node.node_type.value
            result.by_type[node_type] = result.by_type.get(node_type, 0) + 1

        # ê¶Œì¥ ì¡°ì¹˜
        if include_recommendations:
            result.recommendations = self._generate_deletion_recommendations(
                node_id, impacted
            )

        return result

    def analyze_schema_change_impact(
        self,
        node_id: str,
        changed_fields: List[str],
        change_type: str = "modify"  # modify, add, remove
    ) -> ImpactResult:
        """
        ìŠ¤í‚¤ë§ˆ ë³€ê²½ ì˜í–¥ ë¶„ì„

        Args:
            node_id: ë³€ê²½ë˜ëŠ” ë…¸ë“œ ID
            changed_fields: ë³€ê²½ë˜ëŠ” í•„ë“œ ëª©ë¡
            change_type: ë³€ê²½ ìœ í˜•

        Returns:
            ì˜í–¥ ë¶„ì„ ê²°ê³¼
        """
        result = ImpactResult(
            source_node_id=node_id,
            change_type=f"schema_{change_type}"
        )

        # í•„ë“œ ë…¸ë“œ ì°¾ê¸°
        field_nodes = self._find_related_field_nodes(node_id, changed_fields)

        # ê° í•„ë“œë³„ ë‹¤ìš´ìŠ¤íŠ¸ë¦¼ ë¶„ì„
        all_impacted: Dict[str, ImpactedNode] = {}

        for field_node in field_nodes:
            downstream_nodes, downstream_edges = self.tracker.get_downstream(
                field_node.node_id
            )

            impacted = self._analyze_downstream_impact(
                field_node.node_id,
                downstream_nodes,
                downstream_edges,
                change_type=f"field_{change_type}"
            )

            for node in impacted:
                if node.node.node_id not in all_impacted:
                    all_impacted[node.node.node_id] = node
                else:
                    # ë” ë†’ì€ ì˜í–¥ ìˆ˜ì¤€ìœ¼ë¡œ ì—…ë°ì´íŠ¸
                    existing = all_impacted[node.node.node_id]
                    if self._compare_impact_level(node.impact_level, existing.impact_level) > 0:
                        all_impacted[node.node.node_id] = node

        result.impacted_nodes = list(all_impacted.values())
        result.total_impacted = len(result.impacted_nodes)

        # ì§‘ê³„
        for node in result.impacted_nodes:
            level = node.impact_level.value
            result.by_level[level] = result.by_level.get(level, 0) + 1
            node_type = node.node.node_type.value
            result.by_type[node_type] = result.by_type.get(node_type, 0) + 1

        # ê¶Œì¥ ì¡°ì¹˜
        result.recommendations = self._generate_schema_change_recommendations(
            changed_fields, change_type, result.impacted_nodes
        )

        return result

    def analyze_source_unavailability(
        self,
        source_id: str
    ) -> ImpactResult:
        """
        ì†ŒìŠ¤ ë¶ˆê°€ìš© ì˜í–¥ ë¶„ì„

        Args:
            source_id: ì†ŒìŠ¤ ID

        Returns:
            ì˜í–¥ ë¶„ì„ ê²°ê³¼
        """
        # ì†ŒìŠ¤ ë…¸ë“œ ì°¾ê¸°
        source_nodes = self.tracker.find_nodes(
            node_type=NodeType.SOURCE,
            source_id=source_id
        )

        if not source_nodes:
            return ImpactResult(
                source_node_id=source_id,
                change_type="unavailable",
                recommendations=["Source not found in lineage"]
            )

        # ì‚­ì œ ì˜í–¥ ë¶„ì„ê³¼ ë™ì¼í•˜ê²Œ ì²˜ë¦¬
        result = self.analyze_deletion_impact(
            source_nodes[0].node_id,
            include_recommendations=False
        )
        result.change_type = "source_unavailable"

        # ì†ŒìŠ¤ ë¶ˆê°€ìš© íŠ¹í™” ê¶Œì¥ ì¡°ì¹˜
        result.recommendations = [
            "ë°ì´í„° ì†ŒìŠ¤ ê°€ìš©ì„± í™•ì¸ í•„ìš”",
            "ëŒ€ì²´ ë°ì´í„° ì†ŒìŠ¤ ê²€í† ",
            f"ì˜í–¥ë°›ëŠ” íŒŒì´í”„ë¼ì¸ {result.total_impacted}ê°œ ì¼ì‹œ ì¤‘ì§€ ê¶Œì¥"
        ]

        # í¬ë¦¬í‹°ì»¬ ë…¸ë“œê°€ ìˆìœ¼ë©´ ì¶”ê°€ ê¶Œì¥
        critical_count = result.by_level.get("critical", 0)
        if critical_count > 0:
            result.recommendations.insert(0, f"âš ï¸ í¬ë¦¬í‹°ì»¬ ì˜í–¥ {critical_count}ê±´ - ì¦‰ì‹œ ì¡°ì¹˜ í•„ìš”")

        return result

    def get_dependency_report(
        self,
        node_id: str
    ) -> Dict[str, Any]:
        """
        ë…¸ë“œ ì˜ì¡´ì„± ë³´ê³ ì„œ

        Args:
            node_id: ë…¸ë“œ ID

        Returns:
            ì˜ì¡´ì„± ë³´ê³ ì„œ
        """
        # ì—…ìŠ¤íŠ¸ë¦¼ (ì´ ë…¸ë“œê°€ ì˜ì¡´í•˜ëŠ” ê²ƒ)
        upstream_nodes, upstream_edges = self.tracker.get_upstream(node_id)

        # ë‹¤ìš´ìŠ¤íŠ¸ë¦¼ (ì´ ë…¸ë“œì— ì˜ì¡´í•˜ëŠ” ê²ƒ)
        downstream_nodes, downstream_edges = self.tracker.get_downstream(node_id)

        # ê·¸ë˜í”„ êµ¬ì¶•
        graph = LineageGraph()
        for node in upstream_nodes + downstream_nodes:
            graph.add_node(node)
        for edge in upstream_edges + downstream_edges:
            graph.add_edge(edge)

        # í†µê³„
        stats = graph.get_statistics()

        return {
            "node_id": node_id,
            "upstream": {
                "count": len(upstream_nodes),
                "by_type": self._count_by_type(upstream_nodes),
                "nodes": [
                    {"id": n.node_id, "name": n.name, "type": n.node_type.value}
                    for n in upstream_nodes
                ]
            },
            "downstream": {
                "count": len(downstream_nodes),
                "by_type": self._count_by_type(downstream_nodes),
                "nodes": [
                    {"id": n.node_id, "name": n.name, "type": n.node_type.value}
                    for n in downstream_nodes
                ]
            },
            "graph_statistics": stats,
            "visualization": graph.to_visualization_format()
        }

    def get_data_flow_report(
        self,
        source_id: str
    ) -> Dict[str, Any]:
        """
        ì†ŒìŠ¤ë³„ ë°ì´í„° íë¦„ ë³´ê³ ì„œ

        Args:
            source_id: ì†ŒìŠ¤ ID

        Returns:
            ë°ì´í„° íë¦„ ë³´ê³ ì„œ
        """
        lineage = self.tracker.get_source_lineage(source_id)

        if "error" in lineage:
            return lineage

        # ê·¸ë˜í”„ êµ¬ì¶•
        graph = LineageGraph()
        nodes = [LineageNode.from_dict(n) for n in lineage.get("nodes", [])]
        edges = [LineageEdge.from_dict(e) for e in lineage.get("edges", [])]

        for node in nodes:
            graph.add_node(node)
        for edge in edges:
            graph.add_edge(edge)

        # ìœ„ìƒ ì •ë ¬ (ë°ì´í„° íë¦„ ìˆœì„œ)
        flow_order = graph.topological_sort()

        # ìŠ¤í…Œì´ì§€ë³„ ë¶„ë¥˜
        stages = self._classify_stages(nodes, edges)

        return {
            "source_id": source_id,
            "total_nodes": len(nodes),
            "total_edges": len(edges),
            "stages": stages,
            "flow_order": flow_order,
            "cycles_detected": len(graph.detect_cycles()) > 0,
            "graph_statistics": graph.get_statistics(),
            "visualization": graph.to_visualization_format()
        }

    def _analyze_downstream_impact(
        self,
        source_node_id: str,
        downstream_nodes: List[LineageNode],
        downstream_edges: List[LineageEdge],
        change_type: str
    ) -> List[ImpactedNode]:
        """ë‹¤ìš´ìŠ¤íŠ¸ë¦¼ ì˜í–¥ ë¶„ì„"""
        impacted = []

        # ê±°ë¦¬ ê³„ì‚°
        distances = self._calculate_distances(source_node_id, downstream_edges)

        for node in downstream_nodes:
            if node.node_id == source_node_id:
                continue

            distance = distances.get(node.node_id, 99)

            # ì˜í–¥ ìˆ˜ì¤€ ê²°ì •
            impact_level = self._determine_impact_level(node, distance, change_type)

            # ì˜í–¥ ì‚¬ìœ 
            impact_reason = self._generate_impact_reason(node, distance, change_type)

            # ê²½ë¡œ ê³„ì‚°
            path = self._find_shortest_path(
                source_node_id, node.node_id, downstream_edges
            )

            impacted.append(ImpactedNode(
                node=node,
                impact_level=impact_level,
                impact_reason=impact_reason,
                distance_from_source=distance,
                path=path
            ))

        # ì˜í–¥ ìˆ˜ì¤€ìœ¼ë¡œ ì •ë ¬
        impacted.sort(key=lambda x: (
            -self._impact_level_to_int(x.impact_level),
            x.distance_from_source
        ))

        return impacted

    def _determine_impact_level(
        self,
        node: LineageNode,
        distance: int,
        change_type: str
    ) -> ImpactLevel:
        """ì˜í–¥ ìˆ˜ì¤€ ê²°ì •"""
        # ê±°ë¦¬ ê¸°ë°˜ ê¸°ë³¸ ìˆ˜ì¤€
        if distance == 1:
            base_level = ImpactLevel.HIGH
        elif distance <= 3:
            base_level = ImpactLevel.MEDIUM
        else:
            base_level = ImpactLevel.LOW

        # ë…¸ë“œ íƒ€ì…ì— ë”°ë¥¸ ì¡°ì •
        if node.node_type == NodeType.REPORT:
            # ë¦¬í¬íŠ¸ëŠ” ì˜í–¥ì´ ë” í¼
            base_level = self._increase_level(base_level)
        elif node.node_type == NodeType.MODEL:
            # ML ëª¨ë¸ë„ ì˜í–¥ì´ í¼
            base_level = self._increase_level(base_level)

        # ë³€ê²½ ìœ í˜•ì— ë”°ë¥¸ ì¡°ì •
        if change_type == "delete":
            base_level = self._increase_level(base_level)

        return base_level

    def _increase_level(self, level: ImpactLevel) -> ImpactLevel:
        """ì˜í–¥ ìˆ˜ì¤€ ì¦ê°€"""
        order = [ImpactLevel.NONE, ImpactLevel.LOW, ImpactLevel.MEDIUM,
                 ImpactLevel.HIGH, ImpactLevel.CRITICAL]
        idx = order.index(level)
        return order[min(idx + 1, len(order) - 1)]

    def _impact_level_to_int(self, level: ImpactLevel) -> int:
        """ì˜í–¥ ìˆ˜ì¤€ì„ ì •ìˆ˜ë¡œ ë³€í™˜"""
        mapping = {
            ImpactLevel.NONE: 0,
            ImpactLevel.LOW: 1,
            ImpactLevel.MEDIUM: 2,
            ImpactLevel.HIGH: 3,
            ImpactLevel.CRITICAL: 4
        }
        return mapping.get(level, 0)

    def _compare_impact_level(self, a: ImpactLevel, b: ImpactLevel) -> int:
        """ì˜í–¥ ìˆ˜ì¤€ ë¹„êµ (-1, 0, 1)"""
        return self._impact_level_to_int(a) - self._impact_level_to_int(b)

    def _generate_impact_reason(
        self,
        node: LineageNode,
        distance: int,
        change_type: str
    ) -> str:
        """ì˜í–¥ ì‚¬ìœ  ìƒì„±"""
        if change_type == "delete":
            return f"Upstream data source will be deleted (distance: {distance})"
        elif change_type.startswith("field_"):
            return f"Field dependency affected by schema change (distance: {distance})"
        elif change_type == "source_unavailable":
            return f"Data source unavailability (distance: {distance})"
        else:
            return f"Data modification impact (distance: {distance})"

    def _calculate_distances(
        self,
        source_id: str,
        edges: List[LineageEdge]
    ) -> Dict[str, int]:
        """ê±°ë¦¬ ê³„ì‚° (BFS)"""
        adjacency = {}
        for edge in edges:
            if edge.source_node_id not in adjacency:
                adjacency[edge.source_node_id] = []
            adjacency[edge.source_node_id].append(edge.target_node_id)

        distances = {source_id: 0}
        queue = [source_id]

        while queue:
            current = queue.pop(0)
            current_dist = distances[current]

            for neighbor in adjacency.get(current, []):
                if neighbor not in distances:
                    distances[neighbor] = current_dist + 1
                    queue.append(neighbor)

        return distances

    def _find_shortest_path(
        self,
        start_id: str,
        end_id: str,
        edges: List[LineageEdge]
    ) -> List[str]:
        """ìµœë‹¨ ê²½ë¡œ ì°¾ê¸°"""
        adjacency = {}
        for edge in edges:
            if edge.source_node_id not in adjacency:
                adjacency[edge.source_node_id] = []
            adjacency[edge.source_node_id].append(edge.target_node_id)

        visited = {start_id}
        queue = [(start_id, [start_id])]

        while queue:
            current, path = queue.pop(0)

            if current == end_id:
                return path

            for neighbor in adjacency.get(current, []):
                if neighbor not in visited:
                    visited.add(neighbor)
                    queue.append((neighbor, path + [neighbor]))

        return []

    def _find_related_field_nodes(
        self,
        dataset_node_id: str,
        field_names: List[str]
    ) -> List[LineageNode]:
        """ê´€ë ¨ í•„ë“œ ë…¸ë“œ ì°¾ê¸°"""
        field_nodes = []
        all_fields = self.tracker.find_nodes(node_type=NodeType.FIELD)

        for field_node in all_fields:
            dataset = field_node.properties.get("dataset", "")
            if field_node.name in field_names:
                field_nodes.append(field_node)

        return field_nodes

    def _count_by_type(self, nodes: List[LineageNode]) -> Dict[str, int]:
        """íƒ€ì…ë³„ ì¹´ìš´íŠ¸"""
        counts = {}
        for node in nodes:
            node_type = node.node_type.value
            counts[node_type] = counts.get(node_type, 0) + 1
        return counts

    def _classify_stages(
        self,
        nodes: List[LineageNode],
        edges: List[LineageEdge]
    ) -> List[Dict[str, Any]]:
        """ìŠ¤í…Œì´ì§€ë³„ ë¶„ë¥˜"""
        stages = []

        # ì†ŒìŠ¤ ìŠ¤í…Œì´ì§€
        source_nodes = [n for n in nodes if n.node_type == NodeType.SOURCE]
        if source_nodes:
            stages.append({
                "stage": "extract",
                "description": "Data extraction from sources",
                "nodes": [{"id": n.node_id, "name": n.name} for n in source_nodes]
            })

        # ë³€í™˜ ìŠ¤í…Œì´ì§€
        transform_nodes = [n for n in nodes if n.node_type == NodeType.TRANSFORMATION]
        if transform_nodes:
            stages.append({
                "stage": "transform",
                "description": "Data transformations",
                "nodes": [{"id": n.node_id, "name": n.name} for n in transform_nodes]
            })

        # ë°ì´í„°ì…‹ ìŠ¤í…Œì´ì§€
        dataset_nodes = [n for n in nodes if n.node_type == NodeType.DATASET]
        if dataset_nodes:
            stages.append({
                "stage": "load",
                "description": "Data loaded to datasets",
                "nodes": [{"id": n.node_id, "name": n.name} for n in dataset_nodes]
            })

        # ë¦¬í¬íŠ¸ ìŠ¤í…Œì´ì§€
        report_nodes = [n for n in nodes if n.node_type == NodeType.REPORT]
        if report_nodes:
            stages.append({
                "stage": "report",
                "description": "Reports and dashboards",
                "nodes": [{"id": n.node_id, "name": n.name} for n in report_nodes]
            })

        return stages

    def _generate_deletion_recommendations(
        self,
        node_id: str,
        impacted: List[ImpactedNode]
    ) -> List[str]:
        """ì‚­ì œ ê´€ë ¨ ê¶Œì¥ ì¡°ì¹˜ ìƒì„±"""
        recommendations = []

        if not impacted:
            recommendations.append("No downstream dependencies found. Safe to delete.")
            return recommendations

        # í¬ë¦¬í‹°ì»¬ ì˜í–¥
        critical = [n for n in impacted if n.impact_level == ImpactLevel.CRITICAL]
        if critical:
            recommendations.append(
                f"âš ï¸ CRITICAL: {len(critical)} critical dependencies will break. "
                f"Review before deletion."
            )

        # ë†’ì€ ì˜í–¥
        high = [n for n in impacted if n.impact_level == ImpactLevel.HIGH]
        if high:
            recommendations.append(
                f"HIGH IMPACT: {len(high)} nodes with high dependency. "
                f"Consider migration plan."
            )

        # ë¦¬í¬íŠ¸ ì˜í–¥
        reports = [n for n in impacted if n.node.node_type == NodeType.REPORT]
        if reports:
            recommendations.append(
                f"ğŸ“Š {len(reports)} reports will be affected. "
                f"Notify stakeholders before deletion."
            )

        # ëª¨ë¸ ì˜í–¥
        models = [n for n in impacted if n.node.node_type == NodeType.MODEL]
        if models:
            recommendations.append(
                f"ğŸ¤– {len(models)} ML models will be affected. "
                f"Plan retraining strategy."
            )

        recommendations.append(
            f"Total impact: {len(impacted)} downstream nodes. "
            f"Recommended to create backup before deletion."
        )

        return recommendations

    def _generate_schema_change_recommendations(
        self,
        changed_fields: List[str],
        change_type: str,
        impacted: List[ImpactedNode]
    ) -> List[str]:
        """ìŠ¤í‚¤ë§ˆ ë³€ê²½ ê´€ë ¨ ê¶Œì¥ ì¡°ì¹˜ ìƒì„±"""
        recommendations = []

        if change_type == "remove":
            recommendations.append(
                f"âš ï¸ Removing fields: {', '.join(changed_fields)}. "
                f"This may break downstream transformations."
            )

        if change_type == "modify":
            recommendations.append(
                f"Fields being modified: {', '.join(changed_fields)}. "
                f"Verify type compatibility with downstream."
            )

        if impacted:
            recommendations.append(
                f"Schema change affects {len(impacted)} downstream nodes. "
                f"Consider implementing schema versioning."
            )

            # í•„ë“œ ë ˆë²¨ ì˜í–¥
            field_impacts = [n for n in impacted if n.node.node_type == NodeType.FIELD]
            if field_impacts:
                recommendations.append(
                    f"{len(field_impacts)} derived fields will need updates."
                )

        return recommendations
